{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet Fixmatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torchview torchsummary torchvision kornia torchmetrics matplotlib tqdm path graphviz opencv-python scikit-learn optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.distributions.transforms import LowerCholeskyTransform\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# vizualisation\n",
    "import torchsummary\n",
    "\n",
    "# transforms\n",
    "import torchvision.transforms as T\n",
    "import kornia.augmentation as K\n",
    "from kornia.enhance import normalize\n",
    "from torchvision.transforms import RandAugment\n",
    "\n",
    "# metrics\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "# torchvision\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from torchview import draw_graph\n",
    "\n",
    "from IPython.display import display\n",
    "from IPython.core.display import SVG, HTML\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# typing\n",
    "from typing import Callable\n",
    "\n",
    "from utils import plot_images, plot_transform\n",
    "from model import ConvNN, display_model\n",
    "\n",
    "# os\n",
    "import os\n",
    "import path\n",
    "\n",
    "import random\n",
    "import numpy as np \n",
    "\n",
    "# transformations\n",
    "# import transform as T\n",
    "# from randaugment import RandomAugment\n",
    "\n",
    "# typing\n",
    "from typing import Callable, List, Tuple\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_RANDOM_SEED = 2021\n",
    "\n",
    "def seedBasic(seed=DEFAULT_RANDOM_SEED):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "# torch random seed\n",
    "import torch\n",
    "def seedTorch(seed=DEFAULT_RANDOM_SEED):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "      \n",
    "# basic + tensorflow + torch \n",
    "def seedEverything(seed=DEFAULT_RANDOM_SEED):\n",
    "    seedBasic(seed)\n",
    "    seedTorch(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Set device\n",
    "if ((int(torch.__version__.split(\".\")[0]) >= 2) or (int(torch.__version__.split(\".\")[1]) >= 13)) and torch.has_mps:\n",
    "    device = torch.device(\"mps\")\n",
    "elif torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (3, 32, 32)\n",
    "# See Table 4\n",
    "TAU = 0.9 #! 0.95 in the paper\n",
    "LAMBDA_U = 3\n",
    "MU = 4\n",
    "BATCH_SIZE = 64\n",
    "LR = 0.03\n",
    "BETA = 0.9\n",
    "WEIGHT_DECAY = 0.001\n",
    "BETA_DENSITY = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple CNN for CIFAR10\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, return_features=False):\n",
    "        super().__init__()\n",
    "        self.return_features = return_features\n",
    "        self.conv_32 = nn.Conv2d(3, 32, kernel_size=3, padding='same')\n",
    "        self.conv_64 = nn.Conv2d(32, 64, kernel_size=3, padding='same')\n",
    "        self.conv_96 = nn.Conv2d(64, 96, kernel_size=3, padding='same')\n",
    "        self.conv_128 = nn.Conv2d(96, 128, kernel_size=3, padding='same')\n",
    "        self.fc_512 = nn.Linear(512, 512)\n",
    "        self.fc_10 = nn.Linear(512, 10)\n",
    "        self.max_pool = nn.MaxPool2d(2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.conv_32(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.conv_64(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.conv_96(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.conv_128(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.max_pool(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        if self.return_features:\n",
    "            return x\n",
    "\n",
    "        x = self.fc_512(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc_10(x)\n",
    "        # x = self.softmax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "mean: [0.4913966  0.48215377 0.44651437], std: [0.246344   0.24280126 0.26067406]\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "def compute_mean_std(trainLoader) -> Tuple[List[float], List[float]]:\n",
    "    # initialize the list of means and stds\n",
    "    mean, std = torch.zeros(3), torch.zeros(3)\n",
    "\n",
    "    # iterate over the dataset and compute the sum of each channel\n",
    "    for images, _ in trainLoader:\n",
    "        mean += torch.mean(images, dim=[0,2,3])\n",
    "        std += torch.std(images, dim=[0,2,3])\n",
    "    \n",
    "    # compute the mean and std\n",
    "    mean = mean/len(trainLoader)\n",
    "    std = std/len(trainLoader)\n",
    "\n",
    "    return mean, std\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=True, download=True, transform=transform)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "if not os.path.exists('./data/mean.pt'):\n",
    "    mean, std = compute_mean_std(trainloader)\n",
    "    torch.save(mean, 'data/mean.pt')\n",
    "    torch.save(std, 'data/std.pt')\n",
    "else:\n",
    "    mean, std = torch.load('./data/mean.pt'), torch.load('./data/std.pt')\n",
    "\n",
    "# to numpy\n",
    "mean, std = mean.numpy(), std.numpy()\n",
    "\n",
    "print(f\"mean: {mean}, std: {std}\")\n",
    "\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(\n",
    "    root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_models = 'torch_models' \n",
    "if not os.path.exists(torch_models):\n",
    "    os.makedirs(torch_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_full = ConvNN(return_features=True).to(device)\n",
    "model_full.load_state_dict(torch.load(f'{torch_models}/model_100.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ClusteringLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    La couche de classification convertit l'échantillon d'entrée (caractéristique) en une étiquette souple, c'est-à-dire un vecteur qui représente la probabilité d'appartenance de l'échantillon à chaque cluster.\n",
    "    La probabilité d'appartenance de l'échantillon à chaque cluster est calculée avec la distribution t de Student.\n",
    "\n",
    "    # Exemple\n",
    "    ```\n",
    "    model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "\n",
    "    # Arguments\n",
    "    n_clusters : int - Nombre de clusters.\n",
    "    weights : list of PyTorch tensors of shape `(n_clusters, n_features)` - Représente les centres de clusters initiaux.\n",
    "    alpha : float - Paramètre de la distribution t de Student. La valeur par défaut est 1.0.\n",
    "\n",
    "    # Forme de l'entrée\n",
    "    Tenseur 2D avec la forme : `(n_samples, n_features)`.\n",
    "\n",
    "    # Forme en sortie\n",
    "    Tenseur 2D avec la forme : `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0):\n",
    "        super(ClusteringLayer, self).__init__()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "\n",
    "        if weights is not None:\n",
    "            self.weights = nn.Parameter(torch.tensor(weights))\n",
    "        else:\n",
    "            self.weights = nn.Parameter(torch.Tensor(n_clusters))\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            assert self.initial_weights.shape == self.clusters.shape\n",
    "            self.clusters.data.copy_(torch.from_numpy(self.initial_weights))\n",
    "\n",
    "    def forward(self, x):\n",
    "        q = 1.0 / (1.0 + (torch.sum((x.unsqueeze(1) - self.weights) ** 2, dim=2) / self.alpha))\n",
    "        q = q ** ((self.alpha + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, dim=1)).t()\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.cluster import KMeans\n",
    "# Define a simple CNN model\n",
    "class ConvNN_DC(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        model_full = ConvNN(return_features=True).to(device)\n",
    "        model_full.load_state_dict(torch.load(f'{torch_models}/model_100.pth'))\n",
    "\n",
    "        self.features = model_full\n",
    "        self.clustering_layer = ClusteringLayer(n_clusters=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.clustering_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_test = ConvNN().to(device)\n",
    "# model_test.load_state_dict(torch.load(f'{torch_models}/model_100.pth'))\n",
    "\n",
    "# # test model_full\n",
    "# model_full.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in testloader:\n",
    "#         images = normalize(data=images, mean=mean, std=std)\n",
    "\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model_test(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum()\n",
    "\n",
    "# print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_test = ConvNN_DC(return_there=True).to(device)\n",
    "# # model_test.load_state_dict(torch.load(f'{torch_models}/model_100.pth'))\n",
    "\n",
    "# # test model_full\n",
    "# model_full.eval()\n",
    "# with torch.no_grad():\n",
    "#     correct = 0\n",
    "#     total = 0\n",
    "#     for images, labels in testloader:\n",
    "#         images = normalize(data=images, mean=mean, std=std)\n",
    "\n",
    "#         images = images.to(device)\n",
    "#         labels = labels.to(device)\n",
    "#         outputs = model_test(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum()\n",
    "\n",
    "# print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de la distribution cible\n",
    "def target_distribution(q):\n",
    "    # à compléter\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T\n",
    "\n",
    "def retrieve_info(\n",
    "    cluster_labels: np.ndarray,\n",
    "    y_train: np.ndarray) -> np.ndarray:\n",
    "\n",
    "    # Associe l'étiquette la plus probable à chaque groupe dans le modèle KMeans.\n",
    "    # Résultats : dictionnaire des clusters associés à chaque étiquette.\n",
    "\n",
    "    # Initialisation\n",
    "    reference_labels = np.zeros((len(np.unique(cluster_labels)),1))\n",
    "    # Loop pour chaque label \n",
    "    for i in range(len(np.unique(cluster_labels))):\n",
    "        index = np.where(cluster_labels == i, 1, 0)\n",
    "        num = np.bincount(y_train[index==1]).argmax()\n",
    "        reference_labels[i] = num\n",
    "\n",
    "    return reference_labels\n",
    "\n",
    "def correspondance(y_pred_kmeans, y_train):\n",
    "    # Correspondance entre la partition et les classes de la vérité terrain\n",
    "    reference_labels = retrieve_info(y_pred_kmeans, y_train)\n",
    "    number_labels = np.zeros(len(y_pred_kmeans))\n",
    "    for i in range(len(y_pred_kmeans)):\n",
    "        number_labels[i] = reference_labels[y_pred_kmeans[i]]\n",
    "    return np.array(number_labels, dtype=int)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV. Semi-Supervised Learning: Fixmatch with Deep Clustering"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV.1 Fixmatch on 10% train data with Deep Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "seedEverything()\n",
    "\n",
    "model_DC = ConvNN_DC().to(device)\n",
    "optimizer_DC = torch.optim.Adam(model_DC.parameters(), lr=LR)\n",
    "criterion_DC = nn.KLDivLoss(reduction='batchmean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "224571b9d3bd4a559f82097d01046cf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# cluster centers\n",
    "kmeans_cluster_centers = []\n",
    "features_in = []\n",
    "\n",
    "kmeans = KMeans(n_clusters=10, random_state=0, n_init='auto')\n",
    "\n",
    "# train KMeans on batch\n",
    "pbar = tqdm(trainloader, total=len(trainloader), unit=\"batch\")\n",
    "for i, (train_data) in enumerate(pbar):\n",
    "    images, labels = train_data\n",
    "\n",
    "    images = normalize(data=images, mean=mean, std=std)\n",
    "    feat_in = model_DC.features(images.to(device)).detach().cpu().numpy()\n",
    "\n",
    "    kmeans.fit(\n",
    "        feat_in\n",
    "    )\n",
    "    features_in.append(feat_in)\n",
    "    kmeans_cluster_centers.append(kmeans.cluster_centers_)\n",
    "\n",
    "kmeans_cluster_centers = np.concatenate(kmeans_cluster_centers, axis=0)\n",
    "np.save('kmeans_cluster_centers.npy', kmeans_cluster_centers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_cluster_centers = np.load('kmeans_cluster_centers.npy')\n",
    "\n",
    "model_DC.clustering_layer.weights = torch.nn.Parameter(torch.tensor(kmeans_cluster_centers).to(device), requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(10):\n",
    "    pbar = tqdm(trainloader, total=len(trainloader), unit=\"batch\", desc=f\"Epoch {epoch: >5}\")\n",
    "\n",
    "    for i, (train_data) in enumerate(pbar):\n",
    "        images, labels = train_data\n",
    "\n",
    "        # normalize images\n",
    "        images = normalize(data=images, mean=mean, std=std)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer_DC.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        q = model_DC(images.to(device))\n",
    "\n",
    "        p = target_distribution(q) # update target distribution p\n",
    "\n",
    "        # evaluation of performance of clustering\n",
    "        y_pred = torch.argmax(q, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bb0b996f6c4d6a9ef10775fa50eced",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/782 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pbar = tqdm(trainloader, total=len(trainloader), unit=\"batch\")\n",
    "predictions = []\n",
    "\n",
    "for i, (train_data) in enumerate(pbar):\n",
    "    images, labels = train_data\n",
    "\n",
    "    # normalize images\n",
    "    images_norm = normalize(data=images, mean=mean, std=std)\n",
    "\n",
    "    q = model_DC(images_norm.to(device))\n",
    "\n",
    "    p = target_distribution(q)\n",
    "\n",
    "    y_pred = torch.argmax(q, dim=1)   \n",
    "\n",
    "    predictions.append(y_pred)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3079, 4242, 4740, 2982,    0, 2892,    9, 4727, 5951, 3524, 1622, 5723,\n",
       "        3705, 7575,   61,  566, 2697, 6745,    2,    9,    1, 6462, 1239,  123,\n",
       "        3414, 3616,    4, 7174,    6,    8,  933, 3760,  863, 1408,  903,    7,\n",
       "           1, 2935, 7702,    1,  336, 5602, 6729,    9,    0,    9, 4723, 3703,\n",
       "        5666, 6391, 2722, 3104,    4, 7574,    5, 7162,    0,  562, 5158, 4674,\n",
       "        7331, 6543, 5580, 6053], device='cuda:0')"
      ]
     },
     "execution_count": 425,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ite in range(int(maxiter)):\n",
    "    if ite % update_interval == 0:\n",
    "        q = model.predict(x_train)\n",
    "        p = target_distribution(q)\n",
    "\n",
    "        y_pred = q.argmax(dim=1)\n",
    "        if y_train is not None:\n",
    "            y_pred = correspondance(y_pred, y_train)\n",
    "            acc = torch.round(accuracy_score(y_train, y_pred), 5)\n",
    "            loss = torch.round(loss, 5)\n",
    "            print('(Iter , acc)  ', (ite, acc), ' ; loss=', loss)\n",
    "\n",
    "        delta_label = torch.sum(y_pred != y_pred_last).float() / y_pred.shape[0]\n",
    "        y_pred_last = y_pred.clone().detach()\n",
    "        if ite > 0 and delta_label < tol:\n",
    "            print('delta_label ', delta_label, '< tol ', tol)\n",
    "            print('Reached tolerance threshold. Stopping training.')\n",
    "            break\n",
    "\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, x_train.shape[0])]\n",
    "    loss = model.train_on_batch(x=x_train[idx], y=p[idx])\n",
    "    index = index + 1 if (index + 1) * batch_size <= x_train.shape[0] else 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 32, 32])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536: UserWarning: Using a target size (torch.Size([64, 20])) that is different to the input size (torch.Size([64, 512])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (512) must match the size of tensor b (20) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m target[\u001b[39mrange\u001b[39m(target\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]), cluster_labels] \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m     26\u001b[0m \u001b[39m# compute loss\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m loss \u001b[39m=\u001b[39m loss_DC(features_2d, target\u001b[39m.\u001b[39;49mto(device))\n\u001b[1;32m     29\u001b[0m \u001b[39m# backpropagation\u001b[39;00m\n\u001b[1;32m     30\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[0;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[1;32m   3291\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3292\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[1;32m   3295\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[0;34m(*tensors)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[1;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[0;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (20) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "# train deep clustering model using model full as feature extractor\n",
    "num_epochs = 10\n",
    "\n",
    "model_full.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "\n",
    "    for i, (images, labels) in enumerate(trainloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # extract features using model full\n",
    "        features = model_full(images)\n",
    "        feature_dim = np.prod(features.size()[1:])\n",
    "        features_2d = features.view(BATCH_SIZE, feature_dim)\n",
    "\n",
    "\n",
    "        # perform clustering\n",
    "        kmeans = KMeans(n_clusters=20, random_state=0, n_init='auto')\n",
    "        cluster_labels = kmeans.fit_predict(features_2d.detach().cpu().numpy())\n",
    "\n",
    "        # convert cluster labels to one-hot encoding\n",
    "        target = torch.zeros((images.shape[0], 20))\n",
    "        target[range(target.shape[0]), cluster_labels] = 1\n",
    "\n",
    "        # compute loss\n",
    "        loss = loss_DC(features_2d, target.to(device))\n",
    "\n",
    "        # backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(trainloader)\n",
    "    print(f'Epoch {epoch+1} / {num_epochs}, Loss: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5,  5,  5,  2,  5,  5,  5,  5,  8,  5,  7,  5,  5,  5,  5,  4,  5,\n",
       "        5,  4,  4,  9,  0, 16,  4, 13,  4,  5, 12,  4, 19, 15,  4,  5,  3,\n",
       "        4,  5,  4,  7,  4,  5,  5, 17,  9,  5,  0, 18,  6,  4,  5,  4,  4,\n",
       "        0,  4,  5,  9,  5,  4,  1, 11,  9, 10, 14,  5,  7], dtype=int32)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 512])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataset.py:348: UserWarning: Length of split at index 1 is 0. This might result in an empty dataset.\n",
      "  warnings.warn(f\"Length of split at index {i} is 0. \"\n"
     ]
    }
   ],
   "source": [
    "# Define your dataset and dataloaders for labeled and unlabeled data\n",
    "seedEverything()\n",
    "\n",
    "EPOCHS = 50\n",
    "SUBSET_PROP = 0.01\n",
    "K_SAMPLES = 1\n",
    "\n",
    "# 10% labeled data and 100% unlabeled (see note 2 in paper)\n",
    "trainset_sup, _ = torch.utils.data.random_split(trainset, [SUBSET_PROP, 1-SUBSET_PROP])\n",
    "\n",
    "trainset_unsup, _ = torch.utils.data.random_split(trainset, [1, 0])\n",
    "\n",
    "labeled_dataloader = DataLoader(\n",
    "    trainset_sup,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "unlabeled_dataloader = DataLoader(\n",
    "    trainset_unsup,\n",
    "    batch_size=MU*BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# indices of labeled data\n",
    "labeled_indices = trainset_sup.indices\n",
    "\n",
    "# indices of unlabeled data\n",
    "unlabeled_indices = trainset_unsup.indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformations\n",
    "weak_transform = K.ImageSequential(\n",
    "    K.RandomHorizontalFlip(p=0.50), \n",
    "    K.RandomAffine(degrees=0, translate=(0.125, 0.125)),\n",
    ")\n",
    "\n",
    "strong_transform = K.ImageSequential(\n",
    "    K.auto.RandAugment(n=2, m=10),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask(model, weak_unlabeled_data):\n",
    "    with torch.no_grad():\n",
    "        model.train()\n",
    "\n",
    "        qb = model(weak_unlabeled_data)\n",
    "\n",
    "        # qb = logits.copy()\n",
    "        qb = torch.softmax(qb, dim=1)\n",
    "\n",
    "        max_qb, qb_hat = torch.max(qb, dim=1)\n",
    "\n",
    "        idx = max_qb > TAU\n",
    "        qb_hat = qb_hat[idx]\n",
    "\n",
    "    return qb_hat.detach(), idx, max_qb.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNN().to(device)\n",
    "\n",
    "# criterion and optimizer\n",
    "labeled_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "unlabeled_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=BETA, weight_decay=WEIGHT_DECAY, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity(data: torch.tensor) -> torch.tensor:\n",
    "    data_norm = normalize(data=data, mean=mean, std=std)\n",
    "\n",
    "    input1 = data_norm.view(data_norm.shape[0], -1)\n",
    "\n",
    "    cos_sim = torch.tensor(cosine_similarity(input1.cpu().numpy()), dtype=torch.float32).to(device).flatten()\n",
    "    # information_density = (torch.sum(torch.tensor(cos_sim), dim=1) / data_norm.shape[0]) ** BETA_DENSITY\n",
    "\n",
    "    # top k samples\n",
    "    inf_dens, idx = torch.topk(cos_sim, k=1, largest=True, sorted=True)\n",
    "\n",
    "    return inf_dens, idx.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information_density(\n",
    "    model: ConvNN, \n",
    "    unlabeled_inputs: torch.Tensor,\n",
    "    k_samp: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    inputs_norm = normalize(data=unlabeled_inputs, mean=mean, std=std)\n",
    "\n",
    "    logits = model(inputs_norm)\n",
    "    qb = torch.softmax(logits, dim=1)\n",
    "    qb1 = torch.topk(qb, k=2, dim=1)[0][:, 0]\n",
    "    qb2 = torch.topk(qb, k=2, dim=1)[0][:, 1]\n",
    "\n",
    "    uncertainty = 1 - (qb1 - qb2)\n",
    "\n",
    "    input1 = inputs_norm.view(inputs_norm.shape[0], -1)\n",
    "\n",
    "    cos_sim = cosine_similarity(input1.cpu().numpy())\n",
    "    information_density = (torch.sum(torch.tensor(cos_sim), dim=1) / inputs_norm.shape[0]) ** BETA_DENSITY\n",
    "\n",
    "    output = uncertainty * information_density.to(device)\n",
    "\n",
    "    inf_dens, idx = torch.topk(output, k=k_samp, dim=0)\n",
    "    \n",
    "    return inf_dens, idx.cpu().numpy()\n",
    "\n",
    "# Create a new labeled dataset using active learning\n",
    "def create_labeled_dataset_active_learning(dataset, selected_indices):\n",
    "    labeled_dataset = torch.utils.data.Subset(dataset, selected_indices)\n",
    "    return labeled_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c219a7f8d1aa441bb949287dbb5c12a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch     0:   0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information density: 0.10706252604722977\n",
      "Density indices: [171]\n",
      "Test Accuracy: 10.0%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64fd7a828c7486da309c1b25273e393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch     1:   0%|          | 0/8 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[185], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39m# Apply strong augmentation + weak augmentation to unlabeled data\u001b[39;00m\n\u001b[1;32m     64\u001b[0m weak_unlabeled_inputs \u001b[39m=\u001b[39m weak_transform(unlabeled_inputs)\n\u001b[0;32m---> 65\u001b[0m strong_unlabeled_inputs \u001b[39m=\u001b[39m strong_transform(unlabeled_inputs)\n\u001b[1;32m     67\u001b[0m \u001b[39m# normalize\u001b[39;00m\n\u001b[1;32m     68\u001b[0m weak_labeled_inputs \u001b[39m=\u001b[39m normalize(data\u001b[39m=\u001b[39mweak_labeled_inputs, mean\u001b[39m=\u001b[39mmean, std\u001b[39m=\u001b[39mstd)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/container/base.py:273\u001b[0m, in \u001b[0;36mImageSequentialBase.forward\u001b[0;34m(self, input, params, extra_args)\u001b[0m\n\u001b[1;32m    270\u001b[0m     _, out_shape \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mautofill_dim(inp, dim_range\u001b[39m=\u001b[39m(\u001b[39m2\u001b[39m, \u001b[39m4\u001b[39m))\n\u001b[1;32m    271\u001b[0m     params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward_parameters(out_shape)\n\u001b[0;32m--> 273\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_inputs(\u001b[39minput\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams, extra_args\u001b[39m=\u001b[39;49mextra_args)\n\u001b[1;32m    275\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_params \u001b[39m=\u001b[39m params\n\u001b[1;32m    276\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/container/base.py:198\u001b[0m, in \u001b[0;36mImageSequentialBase.transform_inputs\u001b[0;34m(self, input, params, extra_args)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m params:\n\u001b[1;32m    197\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_submodule(param\u001b[39m.\u001b[39mname)\n\u001b[0;32m--> 198\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m InputSequentialOps\u001b[39m.\u001b[39;49mtransform(\u001b[39minput\u001b[39;49m, module\u001b[39m=\u001b[39;49mmodule, param\u001b[39m=\u001b[39;49mparam, extra_args\u001b[39m=\u001b[39;49mextra_args)\n\u001b[1;32m    199\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/container/ops.py:157\u001b[0m, in \u001b[0;36mInputSequentialOps.transform\u001b[0;34m(cls, input, module, param, extra_args)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m, params\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_instance_module_param(param), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_args)\n\u001b[1;32m    156\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, (K\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mImageSequentialBase,)):\n\u001b[0;32m--> 157\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49mtransform_inputs(\u001b[39minput\u001b[39;49m, params\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_sequential_module_param(param), extra_args\u001b[39m=\u001b[39;49mextra_args)\n\u001b[1;32m    158\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, (K\u001b[39m.\u001b[39mauto\u001b[39m.\u001b[39moperations\u001b[39m.\u001b[39mOperationBase,)):\n\u001b[1;32m    159\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m, params\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_instance_module_param(param))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/container/base.py:198\u001b[0m, in \u001b[0;36mImageSequentialBase.transform_inputs\u001b[0;34m(self, input, params, extra_args)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m params:\n\u001b[1;32m    197\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_submodule(param\u001b[39m.\u001b[39mname)\n\u001b[0;32m--> 198\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m InputSequentialOps\u001b[39m.\u001b[39;49mtransform(\u001b[39minput\u001b[39;49m, module\u001b[39m=\u001b[39;49mmodule, param\u001b[39m=\u001b[39;49mparam, extra_args\u001b[39m=\u001b[39;49mextra_args)\n\u001b[1;32m    199\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/container/ops.py:157\u001b[0m, in \u001b[0;36mInputSequentialOps.transform\u001b[0;34m(cls, input, module, param, extra_args)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m, params\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_instance_module_param(param), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mextra_args)\n\u001b[1;32m    156\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, (K\u001b[39m.\u001b[39mcontainer\u001b[39m.\u001b[39mImageSequentialBase,)):\n\u001b[0;32m--> 157\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49mtransform_inputs(\u001b[39minput\u001b[39;49m, params\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_sequential_module_param(param), extra_args\u001b[39m=\u001b[39;49mextra_args)\n\u001b[1;32m    158\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, (K\u001b[39m.\u001b[39mauto\u001b[39m.\u001b[39moperations\u001b[39m.\u001b[39mOperationBase,)):\n\u001b[1;32m    159\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39m, params\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_instance_module_param(param))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/container/base.py:198\u001b[0m, in \u001b[0;36mImageSequentialBase.transform_inputs\u001b[0;34m(self, input, params, extra_args)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m params:\n\u001b[1;32m    197\u001b[0m     module \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_submodule(param\u001b[39m.\u001b[39mname)\n\u001b[0;32m--> 198\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m InputSequentialOps\u001b[39m.\u001b[39;49mtransform(\u001b[39minput\u001b[39;49m, module\u001b[39m=\u001b[39;49mmodule, param\u001b[39m=\u001b[39;49mparam, extra_args\u001b[39m=\u001b[39;49mextra_args)\n\u001b[1;32m    199\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/container/ops.py:159\u001b[0m, in \u001b[0;36mInputSequentialOps.transform\u001b[0;34m(cls, input, module, param, extra_args)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39mtransform_inputs(\u001b[39minput\u001b[39m, params\u001b[39m=\u001b[39m\u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mget_sequential_module_param(param), extra_args\u001b[39m=\u001b[39mextra_args)\n\u001b[1;32m    158\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(module, (K\u001b[39m.\u001b[39mauto\u001b[39m.\u001b[39moperations\u001b[39m.\u001b[39mOperationBase,)):\n\u001b[0;32m--> 159\u001b[0m     \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m, params\u001b[39m=\u001b[39;49m\u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49mget_instance_module_param(param))\n\u001b[1;32m    160\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    161\u001b[0m     \u001b[39mif\u001b[39;00m param\u001b[39m.\u001b[39mdata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/auto/operations/base.py:151\u001b[0m, in \u001b[0;36mOperationBase.forward\u001b[0;34m(self, input, params)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gradient_estimator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[39m# skip the gradient computation if gradient estimator is provided.\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 151\u001b[0m         output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mop(\u001b[39minput\u001b[39;49m, params\u001b[39m=\u001b[39;49mparams)\n\u001b[1;32m    152\u001b[0m     output \u001b[39m=\u001b[39m batch_prob \u001b[39m*\u001b[39m output \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m batch_prob) \u001b[39m*\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m    153\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmagnitude \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    154\u001b[0m         \u001b[39m# If magnitude is None, make the grad w.r.t the input\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/base.py:212\u001b[0m, in \u001b[0;36m_BasicAugmentationBase.forward\u001b[0;34m(self, input, params, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m     params[\u001b[39m'\u001b[39m\u001b[39mbatch_prob\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m tensor([\u001b[39mTrue\u001b[39;00m] \u001b[39m*\u001b[39m batch_shape[\u001b[39m0\u001b[39m])\n\u001b[1;32m    210\u001b[0m params, flags \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_process_kwargs_to_params_and_flags(params, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflags, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 212\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_func(in_tensor, params, flags)\n\u001b[1;32m    213\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_output_tensor(output, input_shape) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeepdim \u001b[39melse\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/_2d/base.py:125\u001b[0m, in \u001b[0;36mRigidAffineAugmentationBase2D.apply_func\u001b[0;34m(self, in_tensor, params, flags)\u001b[0m\n\u001b[1;32m    122\u001b[0m     flags \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mflags\n\u001b[1;32m    124\u001b[0m trans_matrix \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate_transformation_matrix(in_tensor, params, flags)\n\u001b[0;32m--> 125\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform_inputs(in_tensor, params, flags, trans_matrix)\n\u001b[1;32m    126\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_transform_matrix \u001b[39m=\u001b[39m trans_matrix\n\u001b[1;32m    128\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/base.py:263\u001b[0m, in \u001b[0;36m_AugmentationBase.transform_inputs\u001b[0;34m(self, input, params, flags, transform, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m in_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform_tensor(\u001b[39minput\u001b[39m)\n\u001b[1;32m    262\u001b[0m \u001b[39mif\u001b[39;00m to_apply\u001b[39m.\u001b[39mall():\n\u001b[0;32m--> 263\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_transform(in_tensor, params, flags, transform\u001b[39m=\u001b[39;49mtransform)\n\u001b[1;32m    264\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mnot\u001b[39;00m to_apply\u001b[39m.\u001b[39many():\n\u001b[1;32m    265\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_non_transform(in_tensor, params, flags, transform\u001b[39m=\u001b[39mtransform)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/augmentation/_2d/intensity/equalize.py:51\u001b[0m, in \u001b[0;36mRandomEqualize.apply_transform\u001b[0;34m(self, input, params, flags, transform)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_transform\u001b[39m(\n\u001b[1;32m     49\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, params: Dict[\u001b[39mstr\u001b[39m, Tensor], flags: Dict[\u001b[39mstr\u001b[39m, Any], transform: Optional[Tensor] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     50\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m equalize(\u001b[39minput\u001b[39;49m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/utils/image.py:226\u001b[0m, in \u001b[0;36mperform_keep_shape_image.<locals>._wrapper\u001b[0;34m(input, *args, **kwargs)\u001b[0m\n\u001b[1;32m    224\u001b[0m input_shape \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape\n\u001b[1;32m    225\u001b[0m \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m _to_bchw(\u001b[39minput\u001b[39m)  \u001b[39m# view input as (B, C, H, W)\u001b[39;00m\n\u001b[0;32m--> 226\u001b[0m output \u001b[39m=\u001b[39m f(\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    227\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(input_shape) \u001b[39m==\u001b[39m \u001b[39m3\u001b[39m:\n\u001b[1;32m    228\u001b[0m     output \u001b[39m=\u001b[39m output[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/enhance/adjust.py:948\u001b[0m, in \u001b[0;36mequalize\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m    944\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m    945\u001b[0m \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m \u001b[39minput\u001b[39m:\n\u001b[1;32m    946\u001b[0m     \u001b[39m# Assumes RGB for now.  Scales each channel independently\u001b[39;00m\n\u001b[1;32m    947\u001b[0m     \u001b[39m# and then stacks the result.\u001b[39;00m\n\u001b[0;32m--> 948\u001b[0m     scaled_image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([_scale_channel(image[i, :, :]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(image))])\n\u001b[1;32m    949\u001b[0m     res\u001b[39m.\u001b[39mappend(scaled_image)\n\u001b[1;32m    950\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(res)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/enhance/adjust.py:948\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    944\u001b[0m res \u001b[39m=\u001b[39m []\n\u001b[1;32m    945\u001b[0m \u001b[39mfor\u001b[39;00m image \u001b[39min\u001b[39;00m \u001b[39minput\u001b[39m:\n\u001b[1;32m    946\u001b[0m     \u001b[39m# Assumes RGB for now.  Scales each channel independently\u001b[39;00m\n\u001b[1;32m    947\u001b[0m     \u001b[39m# and then stacks the result.\u001b[39;00m\n\u001b[0;32m--> 948\u001b[0m     scaled_image \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack([_scale_channel(image[i, :, :]) \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(image))])\n\u001b[1;32m    949\u001b[0m     res\u001b[39m.\u001b[39mappend(scaled_image)\n\u001b[1;32m    950\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mstack(res)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/enhance/adjust.py:907\u001b[0m, in \u001b[0;36m_scale_channel\u001b[0;34m(im)\u001b[0m\n\u001b[1;32m    905\u001b[0m im \u001b[39m=\u001b[39m im \u001b[39m*\u001b[39m \u001b[39m255.0\u001b[39m\n\u001b[1;32m    906\u001b[0m \u001b[39m# Compute the histogram of the image channel.\u001b[39;00m\n\u001b[0;32m--> 907\u001b[0m histo \u001b[39m=\u001b[39m _torch_histc_cast(im, bins\u001b[39m=\u001b[39;49m\u001b[39m256\u001b[39;49m, \u001b[39mmin\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m, \u001b[39mmax\u001b[39;49m\u001b[39m=\u001b[39;49m\u001b[39m255\u001b[39;49m)\n\u001b[1;32m    908\u001b[0m \u001b[39m# For the purposes of computing the step, filter out the nonzeros.\u001b[39;00m\n\u001b[1;32m    909\u001b[0m nonzero_histo \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mreshape(histo[histo \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m], [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/kornia/utils/helpers.py:126\u001b[0m, in \u001b[0;36m_torch_histc_cast\u001b[0;34m(input, bins, min, max)\u001b[0m\n\u001b[1;32m    122\u001b[0m         dtype \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfloat32\n\u001b[1;32m    123\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39minverse(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mto(dtype))\u001b[39m.\u001b[39mto(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m--> 126\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_torch_histc_cast\u001b[39m(\u001b[39minput\u001b[39m: Tensor, bins: \u001b[39mint\u001b[39m, \u001b[39mmin\u001b[39m: \u001b[39mint\u001b[39m, \u001b[39mmax\u001b[39m: \u001b[39mint\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    127\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Helper function to make torch.histc work with other than fp32/64.\u001b[39;00m\n\u001b[1;32m    128\u001b[0m \n\u001b[1;32m    129\u001b[0m \u001b[39m    The function torch.histc is only implemented for fp32/64 which makes impossible to be used by fp16 or others. What\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m    this function does, is cast input data type to fp32, apply torch.inverse, and cast back to the input dtype.\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39minput\u001b[39m, Tensor):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Start training\")\n",
    "\n",
    "current_prop = SUBSET_PROP\n",
    "target_prop = 0.05\n",
    "max_iter = 300\n",
    "j = 0\n",
    "\n",
    "# Define the cosine learning rate decay function\n",
    "# lr_lambda = lambda step: LR * torch.cos(torch.tensor((7 * torch.pi * (step)) / (16 * max_iter//3))) * 100 / 3\n",
    "\n",
    "# cosine annealing scheduler\n",
    "# lr_lambda = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iter, eta_min=0, last_epoch=-1)\n",
    "\n",
    "# Create a learning rate scheduler with the cosine decay function\n",
    "# scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_iter, eta_min=0, last_epoch=-1)\n",
    "\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "added_samp = 0\n",
    "\n",
    "while j <= max_iter and current_prop <= target_prop:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_n_unlabeled = 0\n",
    "    # max_confidence = 0\n",
    "    moving_avg_pred_labeled = 0\n",
    "    moving_avg_pred_unlabeled = 0\n",
    "\n",
    "\n",
    "    pbar = tqdm(zip(labeled_dataloader, unlabeled_dataloader), total=min(len(labeled_dataloader), len(unlabeled_dataloader)), unit=\"batch\", desc=f\"Epoch {j: >5}\")\n",
    "\n",
    "    for i, (labeled_data, unlabeled_data) in enumerate(pbar):\n",
    "        # Get labeled and unlabeled data\n",
    "        labeled_inputs, labels = labeled_data[0].to(device), labeled_data[1].to(device)\n",
    "        unlabeled_inputs, unlabeled_labels = unlabeled_data[0].to(device), unlabeled_data[1].to(device)\n",
    "        \n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # normalize labeled and unlabeled inputs\n",
    "        labeled_inputs_norm = normalize(data=labeled_inputs, mean=mean, std=std)\n",
    "        unlabeled_inputs_norm = normalize(data=unlabeled_inputs, mean=mean, std=std)\n",
    "\n",
    "        # Forward pass \n",
    "        labeled_outputs_norm = model(labeled_inputs_norm)\n",
    "        unlabeled_outputs_norm = model(unlabeled_inputs_norm)\n",
    "\n",
    "        # Compute moving average of labeled and unlabeled predictions\n",
    "        moving_avg_pred_labeled = (i * moving_avg_pred_labeled + labeled_outputs_norm.shape[0]) / (i + 1)\n",
    "        moving_avg_pred_unlabeled = (i * moving_avg_pred_unlabeled + unlabeled_outputs_norm.shape[0]) / (i + 1)\n",
    "\n",
    "        # ratio\n",
    "        ratio = moving_avg_pred_labeled / moving_avg_pred_unlabeled\n",
    "\n",
    "        # Apply weak augmentation to labeled data\n",
    "        weak_labeled_inputs = weak_transform(labeled_inputs)\n",
    "\n",
    "        # Apply strong augmentation + weak augmentation to unlabeled data\n",
    "        weak_unlabeled_inputs = weak_transform(unlabeled_inputs)\n",
    "        strong_unlabeled_inputs = strong_transform(unlabeled_inputs)\n",
    "\n",
    "        # normalize\n",
    "        weak_labeled_inputs = normalize(data=weak_labeled_inputs, mean=mean, std=std)\n",
    "        weak_unlabeled_inputs = normalize(data=weak_unlabeled_inputs, mean=mean, std=std)\n",
    "        strong_unlabeled_inputs = normalize(data=strong_unlabeled_inputs, mean=mean, std=std)\n",
    "\n",
    "        # prediction on weak augmented unlabeled data\n",
    "        qb = model(weak_unlabeled_inputs)\n",
    "        qb = torch.softmax(qb, dim=1)\n",
    "        qb_norm = qb * ratio\n",
    "\n",
    "        # normalize\n",
    "        qb_tilde = qb_norm / torch.sum(qb_norm, dim=1, keepdim=True)\n",
    "\n",
    "        # compute mask\n",
    "        max_qb_tilde, qb_tilde_hat = torch.max(qb_tilde, dim=1)\n",
    "        idx = max_qb_tilde > TAU\n",
    "\n",
    "        # pseudo labels\n",
    "        pseudo_labels = qb_tilde_hat[idx]\n",
    "\n",
    "        # mask strong augmented unlabeled data\n",
    "        strong_unlabeled_inputs = strong_unlabeled_inputs[idx]\n",
    "\n",
    "        n_labeled, n_unlabeled = weak_labeled_inputs.size(0), strong_unlabeled_inputs.size(0)\n",
    "\n",
    "        if n_unlabeled != 0:\n",
    "            # Concatenate labeled and unlabeled data\n",
    "            inputs_all = torch.cat((weak_labeled_inputs, strong_unlabeled_inputs))\n",
    "            labels_all = torch.cat((labels, pseudo_labels))\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(inputs_all)\n",
    "            # outputs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "            # split labeled and unlabeled outputs\n",
    "            labeled_outputs, unlabeled_outputs = outputs[:n_labeled], outputs[n_labeled:]\n",
    "\n",
    "            # compute losses\n",
    "            labeled_loss = torch.sum(labeled_criterion(labeled_outputs, labels)) / BATCH_SIZE\n",
    "            unlabeled_loss = torch.sum(unlabeled_criterion(unlabeled_outputs, pseudo_labels)) / (MU * BATCH_SIZE)\n",
    "\n",
    "            # compute total loss\n",
    "            loss = labeled_loss + LAMBDA_U * unlabeled_loss\n",
    "\n",
    "            # compute accuracy\n",
    "            total += labels_all.size(0)\n",
    "            correct += (outputs.argmax(dim=1) == labels_all).sum().item()\n",
    "            \n",
    "        else:\n",
    "            # forward pass\n",
    "            labeled_outputs = model(weak_labeled_inputs)\n",
    "            # labeled_outputs = torch.softmax(labeled_outputs, dim=1)\n",
    "\n",
    "            # compute loss\n",
    "            labeled_loss = torch.sum(labeled_criterion(labeled_outputs, labels)) / BATCH_SIZE\n",
    "            unlabeled_loss = torch.tensor(0, device=device)\n",
    "\n",
    "            # compute total loss\n",
    "            loss = labeled_loss + LAMBDA_U * unlabeled_loss\n",
    "\n",
    "            # compute accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (labeled_outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "\n",
    "        # backward pass + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        # update statistics\n",
    "        running_loss += loss.item()\n",
    "        running_n_unlabeled += n_unlabeled \n",
    "\n",
    "        # update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"labeled loss\": labeled_loss.item(),\n",
    "            \"unlabeled loss\": unlabeled_loss.item(),\n",
    "            \"accuracy\": 100 * correct / total,\n",
    "            \"avg confidence\": torch.mean(max_qb_tilde).item(),\n",
    "            \"n_unlabeled\": running_n_unlabeled,\n",
    "            \"current_prop\": current_prop,\n",
    "            \"lr\": optimizer.param_groups[0]['lr']\n",
    "        })\n",
    "\n",
    "    if j < max_iter and j%5 == 0 and current_prop < target_prop:\n",
    "        # compute information density\n",
    "        scores_density, idx_density = information_density(model, unlabeled_inputs, K_SAMPLES)\n",
    "\n",
    "        # add selected indices to labeled indices\n",
    "        labeled_indices = np.concatenate((labeled_indices, idx_density))\n",
    "\n",
    "        # unique\n",
    "        labeled_indices = np.unique(labeled_indices)\n",
    "\n",
    "        # create new dataset from labeled indices\n",
    "        trainset_sup = create_labeled_dataset_active_learning(trainset, labeled_indices)\n",
    "\n",
    "        # update dataloader\n",
    "        labeled_dataloader = torch.utils.data.DataLoader(trainset_sup, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "\n",
    "        current_prop = len(trainset_sup) / len(trainset)\n",
    "\n",
    "        print(f'Information density: {scores_density.mean()}')\n",
    "        print(f'Density indices: {idx_density}')\n",
    "\n",
    "    # update loss\n",
    "    train_losses.append(running_loss / (i + 1))\n",
    "\n",
    "    # scheduler step\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # normalize\n",
    "            images = normalize(data=images, mean=mean, std=std)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_accuracy = 100.0 * test_correct / test_total\n",
    "        print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "        # update loss\n",
    "        test_losses.append(torch.sum(labeled_criterion(outputs, labels)).item() / BATCH_SIZE)\n",
    "\n",
    "    # increment iteration\n",
    "    j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([114,  49,  26,   9, 237,  70, 171, 127, 200, 236, 172,  89,  16,\n",
       "        37, 208, 243, 140, 119,  73,   8, 166, 203,  64, 207, 196, 108,\n",
       "       116,   0, 222,  38,  91,  74, 111,   4, 255, 205, 126,  18,  84,\n",
       "       194, 135, 132, 195,  63, 163,  62, 129, 157,  96,  68,  33,  79,\n",
       "       216, 248,  13, 101,  54, 238,  46, 245, 113, 229,  25, 155])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.title('Loss at the end of each epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot confusion matrix\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # normalize\n",
    "        images = normalize(data=images, mean=mean, std=std)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        y_true.append(labels.cpu().numpy())\n",
    "        y_pred.append(predicted.cpu().numpy())\n",
    "    \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # normalize\n",
    "        images = normalize(data=images, mean=mean, std=std)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_accuracy = 100.0 * test_correct / test_total\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), f\"{torch_models}/model_10_fixmatch_AL.pth\")\n",
    "\n",
    "test_image, test_labels = testloader.__iter__().__next__()\n",
    "test_image = test_image.to(device)\n",
    "outputs_test = model(test_image)\n",
    "label_pred_test = outputs_test.argmax(dim=1)\n",
    "\n",
    "# descale the images\n",
    "test_image = test_image#  * torch.tensor(std, device=device).view(1, 3, 1, 1) + torch.tensor(mean, device=device).view(1, 3, 1, 1)\n",
    "\n",
    "fig1 = plot_images(test_image, test_labels, label_pred_test, classes, figure_name=f\"Test score with Fixmatch - {int(SUBSET_PROP*100)}% - {test_accuracy:.2f}% - Active Learning\")\n",
    "fig1.savefig(f\"./figures/test_score_{SUBSET_PROP}_fixmatch_AL.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.2 Fixmatch on 5% train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset and dataloaders for labeled and unlabeled data\n",
    "seedEverything()\n",
    "\n",
    "EPOCHS = 50\n",
    "SUBSET_PROP = 0.05\n",
    "\n",
    "# 10% labeled data and 100% unlabeled (see note 2 in paper)\n",
    "trainset_sup, _ = torch.utils.data.random_split(trainset, [SUBSET_PROP, 1-SUBSET_PROP])\n",
    "\n",
    "trainset_unsup, _ = torch.utils.data.random_split(trainset, [1, 0])\n",
    "\n",
    "labeled_dataloader = DataLoader(\n",
    "    trainset_sup,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "unlabeled_dataloader = DataLoader(\n",
    "    trainset_unsup,\n",
    "    batch_size=MU*BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# transformations\n",
    "weak_transform = K.ImageSequential(\n",
    "    K.RandomHorizontalFlip(p=0.50), \n",
    "    K.RandomAffine(degrees=0, translate=(0.125, 0.125)),\n",
    ")\n",
    "\n",
    "strong_transform = K.ImageSequential(\n",
    "    K.auto.RandAugment(n=2, m=10), # randaugment + cutout\n",
    ")\n",
    "\n",
    "def mask(model, weak_unlabeled_data):\n",
    "    with torch.no_grad():\n",
    "        model.train()\n",
    "\n",
    "        qb = model(weak_unlabeled_data)\n",
    "\n",
    "        # qb = logits.copy()\n",
    "        qb = torch.softmax(qb, dim=1)\n",
    "\n",
    "        max_qb, qb_hat = torch.max(qb, dim=1)\n",
    "\n",
    "        idx = max_qb > TAU\n",
    "        qb_hat = qb_hat[idx]\n",
    "\n",
    "    return qb_hat.detach(), idx, max_qb.detach()\n",
    "\n",
    "model = ConvNN().to(device)\n",
    "\n",
    "# criterion and optimizer\n",
    "labeled_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "unlabeled_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=BETA, weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "\n",
    "# Define the cosine learning rate decay function\n",
    "lr_lambda = lambda step: LR * torch.cos(torch.tensor((7 * torch.pi * (step)) / (16 * EPOCHS))) * 100 / 3\n",
    "\n",
    "# Create a learning rate scheduler with the cosine decay function\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_n_unlabeled = 0\n",
    "    max_confidence = 0\n",
    "\n",
    "\n",
    "    pbar = tqdm(zip(labeled_dataloader, unlabeled_dataloader), total=min(len(labeled_dataloader), len(unlabeled_dataloader)), unit=\"batch\", desc=f\"Epoch {epoch: >5}\")\n",
    "\n",
    "    for i, (labeled_data, unlabeled_data) in enumerate(pbar):\n",
    "        # Get labeled and unlabeled data\n",
    "        labeled_inputs, labels = labeled_data[0].to(device), labeled_data[1].to(device)\n",
    "        unlabeled_inputs, _ = unlabeled_data[0].to(device), unlabeled_data[1].to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply weak augmentation to labeled data\n",
    "        weak_labeled_inputs = weak_transform(labeled_inputs)\n",
    "\n",
    "        # Apply strong augmentation + weak augmentation to unlabeled data\n",
    "        weak_unlabeled_inputs = weak_transform(unlabeled_inputs)\n",
    "        strong_unlabeled_inputs = strong_transform(unlabeled_inputs)\n",
    "\n",
    "        # normalize\n",
    "        weak_labeled_inputs = normalize(data=weak_labeled_inputs, mean=mean, std=std)\n",
    "        weak_unlabeled_inputs = normalize(data=weak_unlabeled_inputs, mean=mean, std=std)\n",
    "        strong_unlabeled_inputs = normalize(data=strong_unlabeled_inputs, mean=mean, std=std)\n",
    "\n",
    "        # Compute mask, confidence\n",
    "        pseudo_labels, idx, max_qb = mask(model, weak_unlabeled_inputs)\n",
    "        strong_unlabeled_inputs = strong_unlabeled_inputs[idx]\n",
    "\n",
    "        n_labeled, n_unlabeled = weak_labeled_inputs.size(0), strong_unlabeled_inputs.size(0)\n",
    "\n",
    "        if n_unlabeled != 0:\n",
    "            # Concatenate labeled and unlabeled data\n",
    "            inputs_all = torch.cat((weak_labeled_inputs, strong_unlabeled_inputs))\n",
    "            labels_all = torch.cat((labels, pseudo_labels))\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(inputs_all)\n",
    "            # outputs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "            # split labeled and unlabeled outputs\n",
    "            labeled_outputs, unlabeled_outputs = outputs[:n_labeled], outputs[n_labeled:]\n",
    "\n",
    "            # compute losses\n",
    "            labeled_loss = torch.sum(labeled_criterion(labeled_outputs, labels)) / BATCH_SIZE\n",
    "            unlabeled_loss = torch.sum(unlabeled_criterion(unlabeled_outputs, pseudo_labels)) / (MU * BATCH_SIZE)\n",
    "\n",
    "            # compute total loss\n",
    "            loss = labeled_loss + LAMBDA_U * unlabeled_loss\n",
    "\n",
    "            # compute accuracy\n",
    "            total += labels_all.size(0)\n",
    "            correct += (outputs.argmax(dim=1) == labels_all).sum().item()\n",
    "            \n",
    "        else:\n",
    "            # forward pass\n",
    "            labeled_outputs = model(weak_labeled_inputs)\n",
    "            # labeled_outputs = torch.softmax(labeled_outputs, dim=1)\n",
    "\n",
    "            # compute loss\n",
    "            labeled_loss = torch.sum(labeled_criterion(labeled_outputs, labels)) / BATCH_SIZE\n",
    "            unlabeled_loss = torch.tensor(0, device=device)\n",
    "\n",
    "            # compute total loss\n",
    "            loss = labeled_loss + LAMBDA_U * unlabeled_loss\n",
    "\n",
    "            # compute accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (labeled_outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "\n",
    "        # backward pass + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        # update statistics\n",
    "        running_loss += loss.item()\n",
    "        running_n_unlabeled += n_unlabeled\n",
    "        max_confidence = max(max_confidence, max_qb.max().item())\n",
    "\n",
    "        \n",
    "\n",
    "        # update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"total loss\": loss.item(),\n",
    "            \"labeled loss\": labeled_loss.item(),\n",
    "            \"unlabeled loss\": unlabeled_loss.item(),\n",
    "            \"accuracy\": 100 * correct / total,\n",
    "            \"confidence\": max_confidence,\n",
    "            \"n_unlabeled\": running_n_unlabeled,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'].item()\n",
    "        })\n",
    "\n",
    "    # update loss\n",
    "    train_losses.append(running_loss / (i + 1))\n",
    "\n",
    "    # scheduler step\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # normalize\n",
    "            images = normalize(data=images, mean=mean, std=std)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_accuracy = 100.0 * test_correct / test_total\n",
    "        print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "        # update loss\n",
    "        test_losses.append(torch.sum(labeled_criterion(outputs, labels)).item() / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.title('Loss at the end of each epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot confusion matrix\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # normalize\n",
    "        images = normalize(data=images, mean=mean, std=std)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        y_true.append(labels.cpu().numpy())\n",
    "        y_pred.append(predicted.cpu().numpy())\n",
    "    \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # normalize\n",
    "        images = normalize(data=images, mean=mean, std=std)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_accuracy = 100.0 * test_correct / test_total\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), f\"{torch_models}/model_5_fixmatch.pth\")\n",
    "\n",
    "test_image, test_labels = testloader.__iter__().__next__()\n",
    "test_image = test_image.to(device)\n",
    "outputs_test = model(test_image)\n",
    "label_pred_test = outputs_test.argmax(dim=1)\n",
    "\n",
    "# descale the images\n",
    "test_image = test_image#  * torch.tensor(std, device=device).view(1, 3, 1, 1) + torch.tensor(mean, device=device).view(1, 3, 1, 1)\n",
    "\n",
    "fig1 = plot_images(test_image, test_labels, label_pred_test, classes, figure_name=f\"Test score with Fixmatch - {int(SUBSET_PROP*100)}% - {test_accuracy:.2f}%\")\n",
    "fig1.savefig(f\"./figures/test_score_{SUBSET_PROP}_fixmatch.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III.3 Fixmatch on 1% train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your dataset and dataloaders for labeled and unlabeled data\n",
    "seedEverything()\n",
    "\n",
    "EPOCHS = 50\n",
    "SUBSET_PROP = 0.01\n",
    "\n",
    "# 10% labeled data and 100% unlabeled (see note 2 in paper)\n",
    "trainset_sup, _ = torch.utils.data.random_split(trainset, [SUBSET_PROP, 1-SUBSET_PROP])\n",
    "\n",
    "trainset_unsup, _ = torch.utils.data.random_split(trainset, [1, 0])\n",
    "\n",
    "labeled_dataloader = DataLoader(\n",
    "    trainset_sup,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "unlabeled_dataloader = DataLoader(\n",
    "    trainset_unsup,\n",
    "    batch_size=MU*BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "# transformations\n",
    "weak_transform = K.ImageSequential(\n",
    "    K.RandomHorizontalFlip(p=0.50), \n",
    "    K.RandomAffine(degrees=0, translate=(0.125, 0.125)),\n",
    ")\n",
    "\n",
    "strong_transform = K.ImageSequential(\n",
    "    K.auto.RandAugment(n=2, m=10), # randaugment + cutout\n",
    ")\n",
    "\n",
    "def mask(model, weak_unlabeled_data):\n",
    "    with torch.no_grad():\n",
    "        model.train()\n",
    "\n",
    "        qb = model(weak_unlabeled_data)\n",
    "\n",
    "        # qb = logits.copy()\n",
    "        qb = torch.softmax(qb, dim=1)\n",
    "\n",
    "        max_qb, qb_hat = torch.max(qb, dim=1)\n",
    "\n",
    "        idx = max_qb > TAU\n",
    "        qb_hat = qb_hat[idx]\n",
    "\n",
    "    return qb_hat.detach(), idx, max_qb.detach()\n",
    "\n",
    "model = ConvNN().to(device)\n",
    "\n",
    "# criterion and optimizer\n",
    "labeled_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "unlabeled_criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR, momentum=BETA, weight_decay=WEIGHT_DECAY, nesterov=True)\n",
    "# scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "\n",
    "# Define the cosine learning rate decay function\n",
    "lr_lambda = lambda step: LR * torch.cos(torch.tensor((7 * torch.pi * (step)) / (16 * EPOCHS))) * 100 / 3\n",
    "\n",
    "# Create a learning rate scheduler with the cosine decay function\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)\n",
    "\n",
    "# scheduler = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training\")\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    running_n_unlabeled = 0\n",
    "    max_confidence = 0\n",
    "\n",
    "\n",
    "    pbar = tqdm(zip(labeled_dataloader, unlabeled_dataloader), total=min(len(labeled_dataloader), len(unlabeled_dataloader)), unit=\"batch\", desc=f\"Epoch {epoch: >5}\")\n",
    "\n",
    "    for i, (labeled_data, unlabeled_data) in enumerate(pbar):\n",
    "        # Get labeled and unlabeled data\n",
    "        labeled_inputs, labels = labeled_data[0].to(device), labeled_data[1].to(device)\n",
    "        unlabeled_inputs, _ = unlabeled_data[0].to(device), unlabeled_data[1].to(device)\n",
    "        \n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Apply weak augmentation to labeled data\n",
    "        weak_labeled_inputs = weak_transform(labeled_inputs)\n",
    "\n",
    "        # Apply strong augmentation + weak augmentation to unlabeled data\n",
    "        weak_unlabeled_inputs = weak_transform(unlabeled_inputs)\n",
    "        strong_unlabeled_inputs = strong_transform(unlabeled_inputs)\n",
    "\n",
    "        # normalize\n",
    "        weak_labeled_inputs = normalize(data=weak_labeled_inputs, mean=mean, std=std)\n",
    "        weak_unlabeled_inputs = normalize(data=weak_unlabeled_inputs, mean=mean, std=std)\n",
    "        strong_unlabeled_inputs = normalize(data=strong_unlabeled_inputs, mean=mean, std=std)\n",
    "\n",
    "        # Compute mask, confidence\n",
    "        pseudo_labels, idx, max_qb = mask(model, weak_unlabeled_inputs)\n",
    "        strong_unlabeled_inputs = strong_unlabeled_inputs[idx]\n",
    "\n",
    "        n_labeled, n_unlabeled = weak_labeled_inputs.size(0), strong_unlabeled_inputs.size(0)\n",
    "\n",
    "        if n_unlabeled != 0:\n",
    "            # Concatenate labeled and unlabeled data\n",
    "            inputs_all = torch.cat((weak_labeled_inputs, strong_unlabeled_inputs))\n",
    "            labels_all = torch.cat((labels, pseudo_labels))\n",
    "\n",
    "            # forward pass\n",
    "            outputs = model(inputs_all)\n",
    "            # outputs = torch.softmax(outputs, dim=1)\n",
    "\n",
    "            # split labeled and unlabeled outputs\n",
    "            labeled_outputs, unlabeled_outputs = outputs[:n_labeled], outputs[n_labeled:]\n",
    "\n",
    "            # compute losses\n",
    "            labeled_loss = torch.sum(labeled_criterion(labeled_outputs, labels)) / BATCH_SIZE\n",
    "            unlabeled_loss = torch.sum(unlabeled_criterion(unlabeled_outputs, pseudo_labels)) / (MU * BATCH_SIZE)\n",
    "\n",
    "            # compute total loss\n",
    "            loss = labeled_loss + LAMBDA_U * unlabeled_loss\n",
    "\n",
    "            # compute accuracy\n",
    "            total += labels_all.size(0)\n",
    "            correct += (outputs.argmax(dim=1) == labels_all).sum().item()\n",
    "            \n",
    "        else:\n",
    "            # forward pass\n",
    "            labeled_outputs = model(weak_labeled_inputs)\n",
    "            # labeled_outputs = torch.softmax(labeled_outputs, dim=1)\n",
    "\n",
    "            # compute loss\n",
    "            labeled_loss = torch.sum(labeled_criterion(labeled_outputs, labels)) / BATCH_SIZE\n",
    "            unlabeled_loss = torch.tensor(0, device=device)\n",
    "\n",
    "            # compute total loss\n",
    "            loss = labeled_loss + LAMBDA_U * unlabeled_loss\n",
    "\n",
    "            # compute accuracy\n",
    "            total += labels.size(0)\n",
    "            correct += (labeled_outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "\n",
    "        # backward pass + optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "\n",
    "        # update statistics\n",
    "        running_loss += loss.item()\n",
    "        running_n_unlabeled += n_unlabeled\n",
    "        max_confidence = max(max_confidence, max_qb.max().item())\n",
    "\n",
    "        \n",
    "\n",
    "        # update progress bar\n",
    "        pbar.set_postfix({\n",
    "            \"total loss\": loss.item(),\n",
    "            \"labeled loss\": labeled_loss.item(),\n",
    "            \"unlabeled loss\": unlabeled_loss.item(),\n",
    "            \"accuracy\": 100 * correct / total,\n",
    "            \"confidence\": max_confidence,\n",
    "            \"n_unlabeled\": running_n_unlabeled,\n",
    "            \"lr\": optimizer.param_groups[0]['lr'].item()\n",
    "        })\n",
    "\n",
    "    # update loss\n",
    "    train_losses.append(running_loss / (i + 1))\n",
    "\n",
    "    # scheduler step\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data[0].to(device), data[1].to(device)\n",
    "            # normalize\n",
    "            images = normalize(data=images, mean=mean, std=std)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = outputs.max(1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += predicted.eq(labels).sum().item()\n",
    "        \n",
    "        test_accuracy = 100.0 * test_correct / test_total\n",
    "        print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "        # update loss\n",
    "        test_losses.append(torch.sum(labeled_criterion(outputs, labels)).item() / BATCH_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot losses\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.title('Loss at the end of each epoch')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# plot confusion matrix\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # normalize\n",
    "        images = normalize(data=images, mean=mean, std=std)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "        y_true.append(labels.cpu().numpy())\n",
    "        y_pred.append(predicted.cpu().numpy())\n",
    "    \n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    cm = confusion_matrix(y_true, y_pred, normalize='true')\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on the test set\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data[0].to(device), data[1].to(device)\n",
    "        # normalize\n",
    "        images = normalize(data=images, mean=mean, std=std)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = outputs.max(1)\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "test_accuracy = 100.0 * test_correct / test_total\n",
    "print(f'Test Accuracy: {test_accuracy}%')\n",
    "\n",
    "# save the model\n",
    "torch.save(model.state_dict(), f\"{torch_models}/model_1_fixmatch.pth\")\n",
    "\n",
    "test_image, test_labels = testloader.__iter__().__next__()\n",
    "test_image = test_image.to(device)\n",
    "outputs_test = model(test_image)\n",
    "label_pred_test = outputs_test.argmax(dim=1)\n",
    "\n",
    "# descale the images\n",
    "test_image = test_image#  * torch.tensor(std, device=device).view(1, 3, 1, 1) + torch.tensor(mean, device=device).view(1, 3, 1, 1)\n",
    "\n",
    "fig1 = plot_images(test_image, test_labels, label_pred_test, classes, figure_name=f\"Test score with Fixmatch - {int(SUBSET_PROP*100)}% - {test_accuracy:.2f}%\")\n",
    "fig1.savefig(f\"./figures/test_score_{SUBSET_PROP}_fixmatch.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
